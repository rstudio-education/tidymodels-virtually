---
title: "Build a model"
subtitle: "Tidymodels, Virtually"
session: 01
author: Alison Hill
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "assets/css/my-theme.css", "assets/css/my-fonts.css"]
    seal: false 
    lib_dir: libs
    nature:
      highlightLanguage: "r"
      highlightStyle: "xcode"
      slideNumberFormat: "" 
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes: 
      in_header:
        - 'assets/header.html'
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(comment = ">",
                      message = FALSE,
                      warning = FALSE, 
                      collapse = TRUE,
                      fig.retina = 3,
                      fig.align = 'center',
                      fig.path = "figs/01-model/",
                      R.options = list(tibble.max_extra_cols=5, 
                                       tibble.print_max=5, 
                                       tibble.width=60))
options("scipen" = 16)
library(tidymodels)
yt_counter <- 0
```

```{r packages, include=FALSE}
library(countdown)
library(tidyverse)
library(tidymodels)
library(workflows)
library(scico)
library(gganimate)
library(AmesHousing)
library(tune)
library(viridis)
ames <- make_ames()
theme_set(theme_minimal())

# for figures
train_color <- viridis(1, option="magma", begin = .4)
test_color  <- viridis(1, option="magma", begin = .7)
data_color  <- viridis(1, option="magma", begin = .1)
assess_color <- viridis(1, option="magma", begin = 1)
splits_pal <- c(data_color, train_color, test_color)
```


layout: true

<a class="footer-link" href="TBD">TBD</a>

---

class: title-slide, center, bottom

# `r rmarkdown::metadata$title`

## `r rmarkdown::metadata$subtitle`: `r rmarkdown::metadata$session`

### `r rmarkdown::metadata$author` 


---
name: clouds
class: center, middle
background-image: url(images/Clouds.jpg)
background-size: cover

---
name: clouds2
background-image: url(images/Clouds2.jpg)
background-size: cover

---
template: clouds2
class: middle, center


### Alison Hill 

<img style="border-radius: 50%;" src="https://conf20-intro-ml.netlify.com/authors/alison/avatar.jpg" width="150px"/>

[`r icon::fa("github")` @apreshill](https://github.com/apreshill)  
[`r icon::fa("twitter")` @apreshill](https://twitter.com/apreshill)



???

My name is Alison Hill, and I'm a data scientist and professional educator at RStudio.


---
class: middle, center, frame

# Goal of Predictive Modeling

--


## `r emo::ji("hammer")` build .display[models] that

--


## `r emo::ji("target")` generate .display[accurate predictions]

--


## `r emo::ji("crystal_ball")` for .display[future, yet-to-be-seen data]



--

.footnote[Max Kuhn & Kjell Johnston, http://www.feat.engineering/]


???

This is our whole game vision for today. This is the main goal for predictive modeling broadly, and for machine learning specifically.

We'll use this goal to drive learning of 3 core tidymodels packages:

- parsnip
- yardstick
- and rsample

---
class: inverse, middle, center

# `r emo::ji("hammer")` Build models 

--

## with parsnip


???

Enter the parsnip package

---
exclude: true
name: predictions
class: middle, center, frame

# Goal of Predictive Modeling

## `r emo::ji("crystal_ball")` generate accurate .display[predictions]

---
class: middle

# .center[`lm()`]


```{r}
lm_ames <- lm(Sale_Price ~ Gr_Liv_Area, data = ames)
lm_ames
```


???

So let's start with prediction. To predict, we have to have two things: a model to generate predictions, and data to predict

This type of formula interface may look familiar

How would we use parsnip to build this kind of linear regression model?

---
exclude: true
name: step1
background-image: url("images/predicting/predicting.001.jpeg")
background-size: contain

---
class: middle, frame


# .center[To specify a model with parsnip]

.right-column[

1\. Pick a .display[model]

2\. Set the .display[engine]

3\. Set the .display[mode] (if needed)

]

---
class: middle, frame

# .center[To specify a model with parsnip]



```{r eval = FALSE}
decision_tree() %>%
  set_engine("C5.0") %>%
  set_mode("classification")
```




---
class: middle, frame

# .center[To specify a model with parsnip]


```{r eval = FALSE}
nearest_neighbor() %>%              
  set_engine("kknn") %>%             
  set_mode("regression") %>%        
```



---
class: middle, frame

.fade[
# .center[To specify a model with parsnip]
]


.right-column[

1\. Pick a .display[model]
.fade[
2\. Set the .display[engine]

3\. Set the .display[mode] (if needed)
]

]

---
class: middle, center

# 1\. Pick a .display[model] 

All available models are listed at

<https://tidymodels.github.io/parsnip/articles/articles/Models.html>

```{r echo=FALSE}
knitr::include_url("https://tidymodels.github.io/parsnip/articles/articles/Models.html")
```

---
class: middle

.center[
# `linear_reg()`

Specifies a model that uses linear regression
]

```{r results='hide'}
linear_reg(mode = "regression", penalty = NULL, mixture = NULL)
```

---
class: middle

.center[
# `linear_reg()`

Specifies a model that uses linear regression
]

```{r results='hide'}
linear_reg(
  mode = "regression", # "default" mode, if exists
  penalty = NULL,      # model hyper-parameter
  mixture = NULL       # model hyper-parameter
  )
```

---
class: middle, frame

.fade[
# .center[To specify a model with parsnip]
]


.right-column[
.fade[
1\. Pick a .display[model]
]

2\. Set the .display[engine]

.fade[
3\. Set the .display[mode] (if needed)
]

]

---
class: middle, center


# `set_engine()`

Adds an engine to power or implement the model.


```{r eval=FALSE}
lm_spec %>% set_engine(engine = "lm", ...)
```

---
class: middle, frame

.fade[
# .center[To specify a model with parsnip]
]


.right-column[
.fade[
1\. Pick a .display[model]

2\. Set the .display[engine]
]

3\. Set the .display[mode] (if needed)


]

---
class: middle, center


# `set_mode()`

Sets the class of problem the model will solve, which influences which output is collected. Not necessary if mode is set in Step 1.


```{r eval=FALSE}
lm_spec %>% set_mode(mode = "regression")
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Write a pipe that creates a model that uses `lm()` to fit a linear regression. Save it as `lm_spec` and look at the object. What does it return?


*Hint: you'll need https://tidymodels.github.io/parsnip/articles/articles/Models.html*


```{r echo = FALSE}
countdown(minutes = 3)
```

---


```{r}
lm_spec <- 
   linear_reg() %>%          # model type
   set_engine(engine = "lm") # engine

lm_spec
```

---
class: middle, center

# `fit()`

Train a model by fitting a model. Returns a parsnip model fit.

```{r results='hide'}
fit(lm_spec, Sale_Price ~ Gr_Liv_Area, data = ames)
```

---
class: middle

.center[
# `fit()`

Train a model by fitting a model. Returns a parsnip model fit.
]

```{r results='hide'}
fit(
  lm_spec,                  # parsnip model
  Sale_Price ~ Gr_Liv_Area, # a formula
  data = ames               # dataframe
  )
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Double check. Does

```{r parsnip-lm, results='hide'}
lm_fit <- 
  lm_spec %>% 
  fit(Sale_Price ~ Gr_Liv_Area, 
      data = ames)
lm_fit
```

give the same results as

```{r plain-lm, results='hide'}
lm(Sale_Price ~ Gr_Liv_Area, data = ames)
```

```{r echo=FALSE}
countdown(minutes = 2)
```

---
```{r ref.label='plain-lm'}

```

---
```{r}
lm_fit
```

---
name: handout
class: center, middle

data `(x, y)` + model = fitted model

---
class: center, middle

# Show of hands

How many people have used a fitted model to generate .display[predictions] with R?

---
template: step1

---
name: step2
background-image: url("images/predicting/predicting.003.jpeg")
background-size: contain

---
class: middle, center

# `predict()`

Use a fitted model to predict new `y` values from data. Returns a tibble.

```{r eval=FALSE}
predict(lm_fit, new_data = ames) 
```


---

```{r}
lm_fit %>% 
  predict(new_data = ames)
```

---
name: lm-predict
class: middle, center

# Predictions

```{r lm-predict, echo = FALSE, fig.align='center'}
# smaller for plotting
set.seed(0)
small_ames <- ames %>% 
  sample_n(80) %>% 
  mutate(.row = dplyr::row_number())

# split
set.seed(100) # Important!
small_split  <- initial_split(small_ames)
small_train  <- training(small_split)
small_test   <- testing(small_split)

ggplot(small_train, aes(Gr_Liv_Area, Sale_Price)) +
  geom_smooth(method = "lm", se = FALSE, colour = "#4D8DC9") +
  geom_point(size = 3, alpha = .5) +
  geom_point(aes(Gr_Liv_Area, fitted(lm(Sale_Price ~ Gr_Liv_Area))),
             color = "#E7553C",
             size = 3) +
  coord_cartesian(y = c(50000, 500000)) +
  theme(text = element_text(family = "Lato"))
```

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Fill in the blanks. Use `predict()` to

1. Use your linear model to predict sale prices; save the tibble as `price_pred`  
1. Add a pipe and use `mutate()` to add a column with the observed sale prices; name it `truth`

*Hint: Be sure to remove every `_` before running the code!*

```{r echo=FALSE}
countdown(minutes = 2)
```

---

```{r}
lm_fit <- 
  lm_spec %>% 
  fit(Sale_Price ~ Gr_Liv_Area, 
      data = ames)

price_pred <- 
  lm_fit %>% 
  predict(new_data = ames) %>% 
  mutate(truth = ames$Sale_Price)

price_pred
```

---
template: handout

--

data `(x)` + fitted model = predictions

---
template: predictions

---
name: accurate-predictions
class: middle, center, frame

# Goal of Machine Learning

## `r emo::ji("target")` generate .display[accurate predictions]

???

Now we have predictions from our model. What can we do with them? If we already know the truth, that is, the outcome variable that was observed, we can compare them!

---
class: middle, center, frame

# Axiom

Better Model = Better Predictions (Lower error rate)

---
template: lm-predict

---
class: middle, center

# Residuals

```{r lm-resid, echo = FALSE, fig.align='center'}
ggplot(small_train, aes(Gr_Liv_Area, Sale_Price)) +
  geom_segment(aes(x = Gr_Liv_Area, 
                   xend = Gr_Liv_Area, 
                   y = Sale_Price, 
                   yend = predict(lm(Sale_Price ~ Gr_Liv_Area))), 
               colour = "#E7553C") +
  geom_smooth(method = "lm", se = FALSE, colour = "#4D8DC9") +
  geom_point(size = 3) +  
  geom_point(aes(Gr_Liv_Area, fitted(lm(Sale_Price ~ Gr_Liv_Area))),
             color = "#E7553C",
             size = 3) +
  coord_cartesian(y = c(50000, 500000)) +
  theme(text = element_text(family = "Lato"))
```



---
class: middle, center

# RMSE

Root Mean Squared Error - The standard deviation of the residuals about zero.

$$ \sqrt{\frac{1}{n} \sum_{i=1}^n (\hat{y}_i - {y}_i)^2 }$$ 

---
class: middle, center

# `rmse()*`

Calculates the RMSE based on two columns in a dataframe: 

The .display[truth]: ${y}_i$ 

The predicted .display[estimate]: $\hat{y}_i$ 

```{r eval = FALSE}
rmse(data, truth, estimate)
```


.footnote[`*` from `yardstick`]

---

```{r}
lm_fit <- 
  lm_spec %>% 
  fit(Sale_Price ~ Gr_Liv_Area, 
      data = ames)

price_pred <- 
  lm_fit %>% 
  predict(new_data = ames) %>% 
  mutate(price_truth = ames$Sale_Price)

price_pred %>% 
  rmse(truth = price_truth, estimate = .pred) #<<
```

```{r include = FALSE}
rmse_full <- rmse(price_pred, truth = price_truth, estimate = .pred) %>% pull(.estimate)
```

---
template: step1

---
template: step2

---
name: step3
background-image: url("images/predicting/predicting.004.jpeg")
background-size: contain

---
template: handout

--

data `(x)` + fitted model = predictions

--

data `(y)` + predictions = metrics

---
class: middle, center, inverse

# A model doesn't have to be a straight&nbsp;line!

---
exclude: true

```{r include = FALSE}
set.seed(100)
small_split_cv <- mc_cv(small_ames, times = 1)
rt_spec <- 
  decision_tree() %>%          
  set_engine(engine = "rpart") %>% 
  set_mode("regression")

rt_train <- function(rset) {
  rpart::rpart(Sale_Price ~ Gr_Liv_Area, 
               data = analysis(rset))
}

rt_preds <- small_split_cv %>% 
  mutate(tree = map(splits, rt_train)) %>% 
  mutate(.fitted_tree = map(tree, predict)) %>% 
  mutate(train_set = map(splits, analysis)) %>% 
  unnest(c(train_set, .fitted_tree))
```


```{r results = 'hide'}
rt_spec <- 
  decision_tree() %>%          
  set_engine(engine = "rpart") %>% 
  set_mode("regression")

rt_fit     <- rt_spec %>% 
              fit(Sale_Price ~ Gr_Liv_Area, 
                  data = ames)

price_pred <- rt_fit %>% 
              predict(new_data = ames) %>% 
              mutate(price_truth = ames$Sale_Price)

rmse(price_pred, truth = price_truth, estimate = .pred)
```

---
class: middle, center

```{r echo = FALSE}
base_rt_plot <-
  ggplot(rt_preds, aes(x = Gr_Liv_Area, y = Sale_Price)) +
  scale_x_continuous(breaks=seq(-0.5, 0.5, 0.1)) +
  theme(legend.position="none", 
        text = element_text(family = "Lato")) +
  coord_cartesian(y = c(50000, 500000), x = c(700, 2750)) +
  ggtitle("Regression Tree") 

base_rt_plot +
  geom_point(size = 3, alpha = .7) +
  geom_line(aes(x=Gr_Liv_Area, y = .fitted_tree), colour="#CA225E", size=2) 
```

---
class: middle, center

```{r echo = FALSE}
base_rt_plot +
  geom_segment(aes(x = Gr_Liv_Area, 
                   xend = Gr_Liv_Area, 
                   y = Sale_Price, 
                   yend = .fitted_tree), 
               colour = "#0D0887FF") +
  geom_point(size = 3, alpha = .7) +
  geom_line(aes(x=Gr_Liv_Area, y = .fitted_tree), colour="#CA225E", size=2)
```


---
class: middle, inverse, center

# Do you trust it?



---
class: middle, inverse, center

# Overfitting

---

```{r include = FALSE}
overfit <-
  ggplot(small_train, aes(Gr_Liv_Area, Sale_Price)) +
  geom_point(size = 3) +
  coord_cartesian(y = c(50000, 500000)) +
  geom_smooth(method = "lm", se = FALSE, colour = "#4D8DC9", lwd = 1) +
  theme(text = element_text(family = "Lato"))
```

```{r echo = FALSE, fig.align='center'}
overfit +
  stat_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 2), colour = "#E7553C", lwd = 1)
```

---

```{r echo = FALSE, fig.align='center'}
overfit +
  stat_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 5), colour = "#E7553C", lwd = 1)
```


---

```{r echo = FALSE, fig.align='center'}
overfit +
  stat_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 9), colour = "#E7553C", lwd = 1)
```


---

```{r include = FALSE}
single_pt <- filter(small_train, Sale_Price == max(Sale_Price))
```


.pull-left[

```{r echo = FALSE, fig.align='center'}
over1 <- overfit +
  stat_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 5), colour = "#E7553C", lwd = 1) +
  geom_point(data = single_pt, 
             fill = "yellow", 
             size = 5,
             shape = 21)
over1
```

]

.pull-right[
```{r echo = FALSE, fig.align='center'}
over2 <- overfit +
  stat_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 9), colour = "#E7553C", lwd = 1) +
  geom_point(data = single_pt, 
             fill = "yellow", 
             size = 5, 
             shape = 21)
over2
```
]

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

.pull-left[
In your teams, decide which model:

1. Has the smallest residuals  
2. Will have lower prediction error. Why?  
]

.pull-right[
```{r echo=FALSE, out.width='50%'}
over1
over2
```

]

```{r echo=FALSE}
countdown(seconds = 90, minutes = 0)
```


---

```{r echo=FALSE, fig.align='center'}
set.seed(100)
mc_split <- mc_cv(small_ames, times = 1)
poly_rec <- recipe(Sale_Price ~ Gr_Liv_Area, data = small_train) %>% 
  step_poly(Gr_Liv_Area, degree = tune())

library(workflows)
poly_flow <- workflow() %>% 
  add_recipe(poly_rec) %>% 
  add_model(lm_spec)

poly_grid <- expand.grid(degree = 1:12)

mc_grid <- tune_grid(poly_flow, 
                     resamples = mc_split,
                     grid = poly_grid,
                     metrics = metric_set(rmse))

mc_grid %>% 
  autoplot() +
  geom_line() +
  scale_x_continuous(breaks=pretty_breaks()) +
  theme(text = element_text(family = "Lato")) +
  coord_cartesian(y = c(45000, 95000))
```

---

```{r echo=FALSE, message = FALSE, fig.align='center'}
mod <- list()
pred <- list()
rmse <- list()
for (i in 1:12) {
    mod[[i]] <- lm(Sale_Price ~ poly(Gr_Liv_Area, i), small_train)
    pred[[i]] <- predict(mod[[i]])
    rmse[[i]] <- rmse_vec(truth = small_train %>% pull(Sale_Price),
                          estimate = pred[[i]])
}

rmse_poly <- enframe(rmse, name = "degree", value = "train") %>% 
  unnest(cols = c(train))

mc_grid %>% 
  collect_metrics() %>% 
  left_join(rmse_poly) %>% 
  pivot_longer(cols = c(mean, train)) %>% 
  ggplot(aes(x = degree, y = value, colour = name)) +
  geom_line(size = 2) +
  scale_x_continuous(breaks=pretty_breaks()) +
  theme(legend.position="none", 
        text = element_text(family = "Lato")) +
  ylab("rmse") +
  coord_cartesian(y = c(45000, 95000)) +
  scale_colour_viridis_d(option = "magma", begin = .2, end = .7)
```


---
class: middle, center, frame

# Axiom 1

The best way to measure a model's performance at predicting new data is to .display[predict new data].

---
class: middle, center, frame

# Goal of Machine Learning

--


## `r emo::ji("hammer")` build .display[models] that

--


## `r emo::ji("target")` generate .display[accurate predictions]

--


## `r emo::ji("crystal_ball")` for .display[future, yet-to-be-seen data]



--

.footnote[Max Kuhn & Kjell Johnston, http://www.feat.engineering/]


???

But need new data...


---
class: middle, center, frame

# Method #1

## Data splitting

---
class: middle, center

```{r all-split, echo = FALSE, fig.width = 12, fig.height = 3}
set.seed(16)
one_split <- slice(ames, 1:30) %>% 
  initial_split() %>% 
  tidy() %>% 
  add_row(Row = 1:30, Data = "Original") %>% 
  mutate(Data = case_when(
    Data == "Analysis" ~ "Training",
    Data == "Assessment" ~ "Testing",
    TRUE ~ Data
  )) %>% 
  mutate(Data = factor(Data, levels = c("Original", "Training", "Testing")))

all_split <-
  ggplot(one_split, aes(x = Row, y = fct_rev(Data), fill = Data)) + 
  geom_tile(color = "white",
            size = 1) + 
  scale_fill_manual(values = splits_pal, guide = FALSE) +
  theme_minimal() +
  theme(axis.text.y = element_text(size = rel(2)),
        axis.text.x = element_blank(),
        legend.position = "top",
        panel.grid = element_blank(),
        text = element_text(family = "Lato")) +
  coord_equal(ratio = 1) +
  labs(x = NULL, y = NULL)

all_split
```

???


We refer to the group for which we know the outcome, and use to develop the algorithm, as the training set. We refer to the group for which we pretend we donâ€™t know the outcome as the test set.

---
class: center

# `initial_split()`

"Splits" data randomly into a single testing and a single training set.

```{r eval= FALSE}
initial_split(data, prop = 3/4)
```


---

```{r}
ames_split <- initial_split(ames, prop = 0.75)
ames_split
```

???

data splitting

---
class: center

# `training()` and `testing()`

Extract training and testing sets from an rsplit

```{r results='hide'}
training(ames_split)
testing(ames_split)
```

---
```{r R.options = list(tibble.max_extra_cols=5, tibble.print_max=5, tibble.width=60)}
train_set <- training(ames_split) 
train_set
```


---
class: middle, center

# Quiz

Now that we have training and testing sets...

--

Which dataset do you think we use for .display[fitting]?

--

Which do we use for .display[predicting]?

---
template: step1

---
template: step2

---
template: step3
background-image: url("images/predicting/predicting.004.jpeg")
background-size: contain

---
name: holdout-step1
background-image: url("images/predicting/predicting.005.jpeg")
background-size: contain

---
name: holdout-step2
background-image: url("images/predicting/predicting.006.jpeg")
background-size: contain

---
name: holdout-step3
background-image: url("images/predicting/predicting.007.jpeg")
background-size: contain

---
name: holdout-step4
background-image: url("images/predicting/predicting.008.jpeg")
background-size: contain

---
name: holdout
background-image: url("images/predicting/predicting.009.jpeg")
background-size: contain

---
class: your-turn

# Your turn `r (yt_counter <- yt_counter + 1)`

Fill in the blanks. 

Use `initial_split()`, `training()`, `testing()`, `lm()` and `rmse()` to:

1. Split **ames** into training and test sets. Save the rsplit!

1. Extract the training data. Fit a linear model to it. Save the model!

1. Measure the RMSE of your linear model with your test set.  

Keep `set.seed(100)` at the start of your code.

```{r echo=FALSE}
countdown(minutes = 4)
```

---

```{r results='hide'}
set.seed(100) # Important!

ames_split  <- initial_split(ames)
ames_train  <- training(ames_split)
ames_test   <- testing(ames_split)

lm_fit      <- lm_spec %>% 
               fit(Sale_Price ~ Gr_Liv_Area, 
                   data = ames_train)

price_pred  <- lm_fit %>% 
               predict(new_data = ames_test) %>% 
               mutate(price_truth = ames_test$Sale_Price)

rmse(price_pred, truth = price_truth, estimate = .pred)
```

```{r include=FALSE}
price_resid  <- predict(lm_fit, new_data = ames_train) %>% 
  mutate(price_truth = ames_train$Sale_Price)
rmse_train <- rmse(price_resid, truth = price_truth, estimate = .pred) %>% pull(.estimate)
rmse_test  <- rmse(price_pred, truth = price_truth, estimate = .pred) %>% pull(.estimate)
```

RMSE = `r round(rmse_test, 2)`; compare to `r round(rmse_full, 2)`

---
class: middle, center

.pull-left[

### Training RMSE = `r round(rmse_train, 2)`
```{r ref.label='lm-resid', echo=FALSE}

```


]

--

.pull-right[

### Testing RMSE = `r round(rmse_test, 2)`
```{r lm-test-resid, echo=FALSE, message = FALSE, warning = FALSE}
train_lm <- lm(Sale_Price ~ Gr_Liv_Area, data = small_train)

lm_test_pred <- train_lm %>% 
  broom::augment(newdata = small_test) %>% 
  select(Sale_Price, Gr_Liv_Area, .fitted, .row)

ggplot(data = NULL, aes(Gr_Liv_Area, Sale_Price)) +
  geom_segment(data = lm_test_pred,
               aes(x = Gr_Liv_Area, 
                   xend = Gr_Liv_Area, 
                   y = Sale_Price, 
                   yend = .fitted), 
               colour = "#E7553C") +
  geom_smooth(data = small_train, method = "lm", se = FALSE, colour = "#4D8DC9",
              fullrange = TRUE) +
  #geom_smooth(data = small_test, method = "lm", se = FALSE, colour = "#2aa198", lty = 4, fullrange = TRUE) +
  geom_point(data = small_test, size = 3) +
  coord_cartesian(y = c(50000, 500000)) +
  theme(text = element_text(family = "Lato"))
```
]


---
name: holdout-handout
class: center, middle

old data `(x, y)` + model = fitted model

--

new data `(x)` + fitted model = predictions

--

new data `(y)` + predictions = metrics

---
class: middle, center

# Quiz

How much data should you set aside for testing?

--

If .display[testing set] is small, 
performance metrics may be unreliable

--

If .display[training set] is small, model fit may be poor

---
class: middle, center, inverse

# Stratified sampling

```{r include=FALSE}
top_area <- small_ames %>% 
  top_n(20, Gr_Liv_Area) %>% 
  pull(.row)

top_sale <- small_ames %>% 
  top_n(20, Sale_Price) %>% 
  pull(.row)

bot_area <- small_ames %>% 
  top_n(-20, Gr_Liv_Area) %>% 
  pull(.row)

bot_sale <- small_ames %>% 
  top_n(-20, Sale_Price) %>% 
  pull(.row)

tidy_ssplit <- small_split %>% 
  tidy() %>% 
  mutate(Data = case_when(
    Data == "Analysis" ~ "Training",
    Data == "Assessment" ~ "Testing",
    TRUE ~ Data
  )) %>% 
  mutate(Data = factor(Data, levels = c("Training", "Testing"))) %>% 
  left_join(small_ames, by = c("Row" = ".row")) %>% 
  select(Gr_Liv_Area, Sale_Price, Data, Row) %>% 
  mutate(hi_area = if_else(Row %in% top_area, "Testing", "Training"),
         hi_sale = if_else(Row %in% top_sale, "Testing", "Training"),
         lo_sale = if_else(Row %in% bot_sale, "Testing", "Training"),
         lo_area = if_else(Row %in% bot_area, "Testing", "Training"))

split_plots <- 
  ggplot(tidy_ssplit, aes(x = Gr_Liv_Area, y = Sale_Price)) +
  geom_point(aes(fill = Data), size = 5, shape = 21, alpha = .8) +
  theme(legend.position="none", 
        text = element_text(family = "Lato")) +
  scale_fill_viridis_d(option = "magma", begin = .2, end = .7)
```

---

```{r echo=FALSE, fig.align='center'}
split_plots
```

---

```{r echo=FALSE, fig.align='center'}
split_plots +
  geom_point(aes(fill = hi_area), size = 5, shape = 21, alpha = .8)
```

---

```{r echo=FALSE, fig.align='center'}
split_plots +
  geom_point(aes(fill = hi_sale), size = 5, shape = 21, alpha = .8)
```

---

```{r echo=FALSE, fig.align='center'}
split_plots +
  geom_point(aes(fill = lo_sale), size = 5, shape = 21, alpha = .8)
```

---

```{r echo=FALSE, fig.align='center'}
split_plots +
  geom_point(aes(fill = lo_area), size = 5, shape = 21, alpha = .8)
```

---
```{r echo = FALSE, fig.align='center'}
set.seed(100)
small_strata <- initial_split(small_ames, 
                              strata = Sale_Price, 
                              breaks = 4)

strata_split <- small_strata %>% 
  tidy() %>% 
  mutate(Data = case_when(
    Data == "Analysis" ~ "Training",
    Data == "Assessment" ~ "Testing",
    TRUE ~ Data
  )) %>% 
  mutate(Data = factor(Data, levels = c("Training", "Testing"))) %>% 
  left_join(small_ames, by = c("Row" = ".row")) %>% 
  select(Gr_Liv_Area, Sale_Price, Data, Row) %>% 
  mutate(bucket = ntile(Sale_Price, n = 4))

strata_plot <- ggplot(strata_split, aes(x = Gr_Liv_Area, y = Sale_Price)) +
  geom_point(size = 5, shape = 21, alpha = .8) +
  theme(legend.position="none", 
        text = element_text(family = "Lato")) +
  scale_fill_viridis_d(option = "magma", begin = .2, end = .7)

strata_plot
```

---
```{r echo=FALSE, fig.align='center'}
strata_plot +
  geom_point(data = filter(strata_split, bucket == 1), 
             aes(fill = Data), size = 5, shape = 21, alpha = .8)
```

---

```{r echo=FALSE, fig.align='center'}
strata_plot +
  geom_point(data = filter(strata_split, bucket == 2), aes(fill = Data), 
             size = 5, shape = 21, alpha = .8) 
```

---

```{r echo=FALSE, fig.align='center'}
strata_plot +
  geom_point(data = filter(strata_split, bucket == 3), aes(fill = Data), 
             size = 5, shape = 21, alpha = .8) 
```

---

```{r echo=FALSE, fig.align='center'}
strata_plot +
  geom_point(data = filter(strata_split, bucket == 4), aes(fill = Data), 
             size = 5, shape = 21, alpha = .8)
```




---
```{r strata, eval=FALSE}
set.seed(100) # Important!

ames_split  <- initial_split(ames, 
                             strata = Sale_Price, #<<
                             breaks = 4) #<<
ames_train  <- training(ames_split)
ames_test   <- testing(ames_split)

lm_fit      <- lm_spec %>% 
               fit(Sale_Price ~ Gr_Liv_Area, 
                   data = ames_train)

price_pred  <- lm_fit %>% 
               predict(new_data = ames_test) %>% 
               mutate(price_truth = ames_test$Sale_Price)

rmse(price_pred, truth = price_truth, estimate = .pred)
```

