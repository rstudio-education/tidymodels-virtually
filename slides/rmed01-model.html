<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Build a model</title>
    <meta charset="utf-8" />
    <meta name="author" content="Alison Hill" />
    <meta name="date" content="2020-08-26" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
    <!---import JQuery-->

    <script
      src="https://code.jquery.com/jquery-3.4.1.min.js"
      integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
      crossorigin="anonymous">
    </script>

    <!--add parent selector-->

    <script>

    $( document ).ready( function(){

      $( "pre")
          .parents( ".remark-slide-content" )
          .addClass( "code-slide-background" );
      
    });
      
    </script>
    <link rel="stylesheet" href="assets/css/my-theme.css" type="text/css" />
    <link rel="stylesheet" href="assets/css/my-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">







class: title-slide, center, bottom

# Build a model

## Tidymodels, virtually &amp;mdash; Session 01

### Alison Hill 



---
class: center, middle, inverse

# What is Machine Learning?

---
class: middle

# .center[Alzheimer's disease data]

Data from a clinical trial of individuals with well-characterized cognitive impairment, and age-matched control participants.


```r
# install.packages("modeldata")
library(modeldata)
data("ad_data")
alz &lt;- ad_data
```



---
class: middle


```r
glimpse(alz)
# Rows: 333
# Columns: 131
# $ ACE_CD143_Angiotensin_Converti   &lt;dbl&gt; 2.0031003, 1.561â€¦
# $ ACTH_Adrenocorticotropic_Hormon  &lt;dbl&gt; -1.3862944, -1.3â€¦
# $ AXL                              &lt;dbl&gt; 1.09838668, 0.68â€¦
# $ Adiponectin                      &lt;dbl&gt; -5.360193, -5.02â€¦
# $ Alpha_1_Antichymotrypsin         &lt;dbl&gt; 1.7404662, 1.458â€¦
# $ Alpha_1_Antitrypsin              &lt;dbl&gt; -12.631361, -11.â€¦
# $ Alpha_1_Microglobulin            &lt;dbl&gt; -2.577022, -3.24â€¦
# $ Alpha_2_Macroglobulin            &lt;dbl&gt; -72.65029, -154.â€¦
# $ Angiopoietin_2_ANG_2             &lt;dbl&gt; 1.06471074, 0.74â€¦
# $ Angiotensinogen                  &lt;dbl&gt; 2.510547, 2.4572â€¦
# $ Apolipoprotein_A_IV              &lt;dbl&gt; -1.427116, -1.66â€¦
# $ Apolipoprotein_A1                &lt;dbl&gt; -7.402052, -7.04â€¦
# $ Apolipoprotein_A2                &lt;dbl&gt; -0.26136476, -0.â€¦
# $ Apolipoprotein_B                 &lt;dbl&gt; -4.624044, -6.74â€¦
# $ Apolipoprotein_CI                &lt;dbl&gt; -1.2729657, -1.2â€¦
# $ Apolipoprotein_CIII              &lt;dbl&gt; -2.312635, -2.34â€¦
# $ Apolipoprotein_D                 &lt;dbl&gt; 2.0794415, 1.335â€¦
# $ Apolipoprotein_E                 &lt;dbl&gt; 3.7545215, 3.097â€¦
# $ Apolipoprotein_H                 &lt;dbl&gt; -0.15734908, -0.â€¦
# $ B_Lymphocyte_Chemoattractant_BL  &lt;dbl&gt; 2.296982, 1.6731â€¦
# $ BMP_6                            &lt;dbl&gt; -2.200744, -1.72â€¦
# $ Beta_2_Microglobulin             &lt;dbl&gt; 0.69314718, 0.47â€¦
# $ Betacellulin                     &lt;int&gt; 34, 53, 49, 52, â€¦
# $ C_Reactive_Protein               &lt;dbl&gt; -4.074542, -6.64â€¦
# $ CD40                             &lt;dbl&gt; -0.7964147, -1.2â€¦
# $ CD5L                             &lt;dbl&gt; 0.09531018, -0.6â€¦
# $ Calbindin                        &lt;dbl&gt; 33.21363, 25.276â€¦
# $ Calcitonin                       &lt;dbl&gt; 1.3862944, 3.610â€¦
# $ CgA                              &lt;dbl&gt; 397.6536, 465.67â€¦
# $ Clusterin_Apo_J                  &lt;dbl&gt; 3.555348, 3.0445â€¦
# $ Complement_3                     &lt;dbl&gt; -10.36305, -16.1â€¦
# $ Complement_Factor_H              &lt;dbl&gt; 3.573725, 3.6000â€¦
# $ Connective_Tissue_Growth_Factor  &lt;dbl&gt; 0.5306283, 0.587â€¦
# $ Cortisol                         &lt;dbl&gt; 10.0, 12.0, 10.0â€¦
# $ Creatine_Kinase_MB               &lt;dbl&gt; -1.710172, -1.75â€¦
# $ Cystatin_C                       &lt;dbl&gt; 9.041922, 9.0676â€¦
# $ EGF_R                            &lt;dbl&gt; -0.1354543, -0.3â€¦
# $ EN_RAGE                          &lt;dbl&gt; -3.688879, -3.81â€¦
# $ ENA_78                           &lt;dbl&gt; -1.349543, -1.35â€¦
# $ Eotaxin_3                        &lt;int&gt; 53, 62, 62, 44, â€¦
# $ FAS                              &lt;dbl&gt; -0.08338161, -0.â€¦
# $ FSH_Follicle_Stimulation_Hormon  &lt;dbl&gt; -0.6516715, -1.6â€¦
# $ Fas_Ligand                       &lt;dbl&gt; 3.1014922, 2.978â€¦
# $ Fatty_Acid_Binding_Protein       &lt;dbl&gt; 2.5208712, 2.247â€¦
# $ Ferritin                         &lt;dbl&gt; 3.329165, 3.9329â€¦
# $ Fetuin_A                         &lt;dbl&gt; 1.2809338, 1.193â€¦
# $ Fibrinogen                       &lt;dbl&gt; -7.035589, -8.04â€¦
# $ GRO_alpha                        &lt;dbl&gt; 1.381830, 1.3724â€¦
# $ Gamma_Interferon_induced_Monokin &lt;dbl&gt; 2.949822, 2.7217â€¦
# $ Glutathione_S_Transferase_alpha  &lt;dbl&gt; 1.0641271, 0.867â€¦
# $ HB_EGF                           &lt;dbl&gt; 6.559746, 8.7545â€¦
# $ HCC_4                            &lt;dbl&gt; -3.036554, -4.07â€¦
# $ Hepatocyte_Growth_Factor_HGF     &lt;dbl&gt; 0.58778666, 0.53â€¦
# $ I_309                            &lt;dbl&gt; 3.433987, 3.1354â€¦
# $ ICAM_1                           &lt;dbl&gt; -0.1907787, -0.4â€¦
# $ IGF_BP_2                         &lt;dbl&gt; 5.609472, 5.3471â€¦
# $ IL_11                            &lt;dbl&gt; 5.121987, 4.9367â€¦
# $ IL_13                            &lt;dbl&gt; 1.282549, 1.2694â€¦
# $ IL_16                            &lt;dbl&gt; 4.192081, 2.8763â€¦
# $ IL_17E                           &lt;dbl&gt; 5.731246, 6.7058â€¦
# $ IL_1alpha                        &lt;dbl&gt; -6.571283, -8.04â€¦
# $ IL_3                             &lt;dbl&gt; -3.244194, -3.91â€¦
# $ IL_4                             &lt;dbl&gt; 2.484907, 2.3978â€¦
# $ IL_5                             &lt;dbl&gt; 1.09861229, 0.69â€¦
# $ IL_6                             &lt;dbl&gt; 0.26936976, 0.09â€¦
# $ IL_6_Receptor                    &lt;dbl&gt; 0.64279595, 0.43â€¦
# $ IL_7                             &lt;dbl&gt; 4.8050453, 3.705â€¦
# $ IL_8                             &lt;dbl&gt; 1.711325, 1.6755â€¦
# $ IP_10_Inducible_Protein_10       &lt;dbl&gt; 6.242223, 5.6869â€¦
# $ IgA                              &lt;dbl&gt; -6.812445, -6.37â€¦
# $ Insulin                          &lt;dbl&gt; -0.6258253, -0.9â€¦
# $ Kidney_Injury_Molecule_1_KIM_1   &lt;dbl&gt; -1.204295, -1.19â€¦
# $ LOX_1                            &lt;dbl&gt; 1.7047481, 1.526â€¦
# $ Leptin                           &lt;dbl&gt; -1.5290628, -1.4â€¦
# $ Lipoprotein_a                    &lt;dbl&gt; -4.268698, -4.93â€¦
# $ MCP_1                            &lt;dbl&gt; 6.740519, 6.8490â€¦
# $ MCP_2                            &lt;dbl&gt; 1.9805094, 1.808â€¦
# $ MIF                              &lt;dbl&gt; -1.237874, -1.89â€¦
# $ MIP_1alpha                       &lt;dbl&gt; 4.968453, 3.6901â€¦
# $ MIP_1beta                        &lt;dbl&gt; 3.258097, 3.1354â€¦
# $ MMP_2                            &lt;dbl&gt; 4.478566, 3.7814â€¦
# $ MMP_3                            &lt;dbl&gt; -2.207275, -2.46â€¦
# $ MMP10                            &lt;dbl&gt; -3.270169, -3.64â€¦
# $ MMP7                             &lt;dbl&gt; -3.7735027, -5.9â€¦
# $ Myoglobin                        &lt;dbl&gt; -1.89711998, -0.â€¦
# $ NT_proBNP                        &lt;dbl&gt; 4.553877, 4.2195â€¦
# $ NrCAM                            &lt;dbl&gt; 5.003946, 5.2094â€¦
# $ Osteopontin                      &lt;dbl&gt; 5.356586, 6.0038â€¦
# $ PAI_1                            &lt;dbl&gt; 1.00350156, -0.0â€¦
# $ PAPP_A                           &lt;dbl&gt; -2.902226, -2.81â€¦
# $ PLGF                             &lt;dbl&gt; 4.442651, 4.0253â€¦
# $ PYY                              &lt;dbl&gt; 3.218876, 3.1354â€¦
# $ Pancreatic_polypeptide           &lt;dbl&gt; 0.5787808, 0.336â€¦
# $ Prolactin                        &lt;dbl&gt; 0.00000000, -0.5â€¦
# $ Prostatic_Acid_Phosphatase       &lt;dbl&gt; -1.620527, -1.73â€¦
# $ Protein_S                        &lt;dbl&gt; -1.784998, -2.46â€¦
# $ Pulmonary_and_Activation_Regulat &lt;dbl&gt; -0.8439701, -2.3â€¦
# $ RANTES                           &lt;dbl&gt; -6.214608, -6.93â€¦
# $ Resistin                         &lt;dbl&gt; -16.475315, -16.â€¦
# $ S100b                            &lt;dbl&gt; 1.5618560, 1.756â€¦
# $ SGOT                             &lt;dbl&gt; -0.94160854, -0.â€¦
# $ SHBG                             &lt;dbl&gt; -1.897120, -1.56â€¦
# $ SOD                              &lt;dbl&gt; 5.609472, 5.8141â€¦
# $ Serum_Amyloid_P                  &lt;dbl&gt; -5.599422, -6.11â€¦
# $ Sortilin                         &lt;dbl&gt; 4.908629, 5.4787â€¦
# $ Stem_Cell_Factor                 &lt;dbl&gt; 4.174387, 3.7135â€¦
# $ TGF_alpha                        &lt;dbl&gt; 8.649098, 11.331â€¦
# $ TIMP_1                           &lt;dbl&gt; 15.20465, 11.266â€¦
# $ TNF_RII                          &lt;dbl&gt; -0.0618754, -0.3â€¦
# $ TRAIL_R3                         &lt;dbl&gt; -0.1829004, -0.5â€¦
# $ TTR_prealbumin                   &lt;dbl&gt; 2.944439, 2.8332â€¦
# $ Tamm_Horsfall_Protein_THP        &lt;dbl&gt; -3.095810, -3.11â€¦
# $ Thrombomodulin                   &lt;dbl&gt; -1.340566, -1.67â€¦
# $ Thrombopoietin                   &lt;dbl&gt; -0.1026334, -0.6â€¦
# $ Thymus_Expressed_Chemokine_TECK  &lt;dbl&gt; 4.149327, 3.8101â€¦
# $ Thyroid_Stimulating_Hormone      &lt;dbl&gt; -3.863233, -4.82â€¦
# $ Thyroxine_Binding_Globulin       &lt;dbl&gt; -1.4271164, -1.6â€¦
# $ Tissue_Factor                    &lt;dbl&gt; 2.04122033, 2.02â€¦
# $ Transferrin                      &lt;dbl&gt; 3.332205, 2.8903â€¦
# $ Trefoil_Factor_3_TFF3            &lt;dbl&gt; -3.381395, -3.91â€¦
# $ VCAM_1                           &lt;dbl&gt; 3.258097, 2.7080â€¦
# $ VEGF                             &lt;dbl&gt; 22.03456, 18.601â€¦
# $ Vitronectin                      &lt;dbl&gt; -0.04082199, -0.â€¦
# $ von_Willebrand_Factor            &lt;dbl&gt; -3.146555, -3.86â€¦
# $ age                              &lt;dbl&gt; 0.9876238, 0.986â€¦
# $ tau                              &lt;dbl&gt; 6.297754, 6.6592â€¦
# $ p_tau                            &lt;dbl&gt; 4.348108, 4.8599â€¦
# $ Ab_42                            &lt;dbl&gt; 12.019678, 11.01â€¦
# $ male                             &lt;dbl&gt; 0, 0, 1, 0, 0, 1â€¦
# $ Genotype                         &lt;fct&gt; E3E3, E3E4, E3E4â€¦
# $ Class                            &lt;fct&gt; Control, Controlâ€¦
```

---
background-image: url(images/hands.jpg)
background-size: contain
background-position: left
class: middle

.pull-right[

## Alzheimer's disease data

+ N = 333

+ 1 categorical outcome: `Class`

+ 130 predictors

+ 126 protein measurements

+ also: `age`, `male`, `Genotype`

]

---
class: middle, center, inverse

# What is the goal of machine learning?

---
class: middle, center, frame

# Goal

--


## Build .display[models] that

--


## generate .display[accurate predictions]

--


## for .display[future, yet-to-be-seen data].



--

.footnote[Max Kuhn &amp; Kjell Johnston, http://www.feat.engineering/]


???

This is our whole game vision for today. This is the main goal for predictive modeling broadly, and for machine learning specifically.

We'll use this goal to drive learning of 3 core tidymodels packages:

- parsnip
- yardstick
- and rsample

---
class: inverse, middle, center

# ðŸ”¨ Build models 

--

# with parsnip


???

Enter the parsnip package

---
class: middle, center, frame

# parsnip

&lt;iframe src="https://parsnip.tidymodels.org" width="100%" height="400px"&gt;&lt;/iframe&gt;

---
exclude: true
name: predictions
class: middle, center, frame

# Goal of Predictive Modeling

## ðŸ”® generate accurate .display[predictions]

---
class: middle

# .center[`glm()`]



```r
glm(Class ~ tau, family = binomial, data = alz)
# 
# Call:  glm(formula = Class ~ tau, family = binomial, data = alz)
# 
# Coefficients:
# (Intercept)          tau  
#      13.664       -2.148  
# 
# Degrees of Freedom: 332 Total (i.e. Null);  331 Residual
# Null Deviance:	    390.6 
# Residual Deviance: 318.8 	AIC: 322.8
```


???

So let's start with prediction. To predict, we have to have two things: a model to generate predictions, and data to predict

This type of formula interface may look familiar

How would we use parsnip to build this kind of linear regression model?

---
name: step1
background-image: url("images/predicting/predicting.001.jpeg")
background-size: contain

---
class: middle, frame


# .center[To specify a model with parsnip]

.right-column[

1\. Pick a .display[model]

2\. Set the .display[engine]

3\. Set the .display[mode] (if needed)

]

---
class: middle, frame

# .center[To specify a model with parsnip]



```r
logistic_reg() %&gt;%
  set_engine("glm") %&gt;%
  set_mode("classification")
# Logistic Regression Model Specification (classification)
# 
# Computational engine: glm
```

---
class: middle, frame

# .center[To specify a model with parsnip]




```r
decision_tree() %&gt;%
  set_engine("C5.0") %&gt;%
  set_mode("classification")
# Decision Tree Model Specification (classification)
# 
# Computational engine: C5.0
```




---
class: middle, frame

# .center[To specify a model with parsnip]



```r
nearest_neighbor() %&gt;%              
  set_engine("kknn") %&gt;%             
  set_mode("classification")        
# K-Nearest Neighbor Model Specification (classification)
# 
# Computational engine: kknn
```



---
class: middle, frame

.fade[
# .center[To specify a model with parsnip]
]


.right-column[

1\. Pick a .display[model]
.fade[
2\. Set the .display[engine]

3\. Set the .display[mode] (if needed)
]

]

---
class: middle, center

# 1\. Pick a .display[model] 

All available models are listed at

&lt;https://www.tidymodels.org/find/parsnip/&gt;

&lt;iframe src="https://www.tidymodels.org/find/parsnip/" width="100%" height="400px"&gt;&lt;/iframe&gt;

---
class: middle

.center[
# `logistic_reg()`

Specifies a model that uses logistic regression
]


```r
logistic_reg(penalty = NULL, mixture = NULL)
```

---
class: middle

.center[
# `logistic_reg()`

Specifies a model that uses logistic regression
]


```r
logistic_reg(
  mode = "classification", # "default" mode, if exists
  penalty = NULL,      # model hyper-parameter
  mixture = NULL       # model hyper-parameter
  )
```

---
class: middle, frame

.fade[
# .center[To specify a model with parsnip]
]


.right-column[
.fade[
1\. Pick a .display[model]
]

2\. Set the .display[engine]

.fade[
3\. Set the .display[mode] (if needed)
]

]

---
class: middle, center


# `set_engine()`

Adds an engine to power or implement the model.



```r
lm_spec %&gt;% set_engine(engine = "lm", ...)
```

---
class: middle, frame

.fade[
# .center[To specify a model with parsnip]
]


.right-column[
.fade[
1\. Pick a .display[model]

2\. Set the .display[engine]
]

3\. Set the .display[mode] (if needed)


]

---
class: middle, center


# `set_mode()`

Sets the class of problem the model will solve, which influences which output is collected. Not necessary if mode is set in Step 1.



```r
lm_spec %&gt;% set_mode(mode = "regression")
```

---
class: your-turn

# Your turn 1

Run the chunk below and look at the output. Then, edit the code below to create a K-nearest neighbor classification model that uses the `kknn` engine. Save it as `nn_mod` and look at the object. What is different about the output?


```r
lr_mod &lt;- 
  logistic_reg() %&gt;% 
  set_engine(engine = "glm") %&gt;% 
  set_mode("classification")
lr_mod
```


*Hint: you'll need https://tidymodels.github.io/parsnip/articles/articles/Models.html*


<div class="countdown" id="timer_5f472c21" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---



```r
lr_mod
# Logistic Regression Model Specification (classification)
# 
# Computational engine: glm

nn_mod &lt;- 
  nearest_neighbor() %&gt;% 
  set_engine(engine = "kknn") %&gt;% 
  set_mode("classification")
nn_mod
# K-Nearest Neighbor Model Specification (classification)
# 
# Computational engine: kknn
```

---
class: inverse, middle, center

Now we've built a model.

--

But, how do we *use* a model?

--

What does it mean to use a model?

---

Statistical models learn from the data. They typically learn model parameters, which can be useful as values for interpretation and understanding.

---
class: center, middle

# Show of hands

How many people have .display[fitted] a statistical model with R?

---

# A fitted model

.pull-left[

```r
lr_mod %&gt;% 
  fit(Class ~ tau + VEGF, 
      data = alz) %&gt;% 
  broom::tidy()
# # A tibble: 3 x 5
#   term        estimate std.error statistic  p.value
#   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
# 1 (Intercept)    8.97      1.98       4.54 5.61e- 6
# 2 tau           -4.01      0.456     -8.79 1.55e-18
# 3 VEGF           0.934     0.130      7.19 6.38e-13
```
]

.pull-right[

&lt;img src="figs/rmed01-model/unnamed-chunk-17-1.png" width="504" style="display: block; margin: auto;" /&gt;
]

---
class: inverse, middle, center

# "All models are wrong, but some are useful"



&lt;img src="figs/rmed01-model/unnamed-chunk-19-1.png" width="504" style="display: block; margin: auto;" /&gt;
]


---
class: inverse, middle, center

# "All models are wrong, but some are useful"



&lt;img src="figs/rmed01-model/unnamed-chunk-21-1.png" width="504" style="display: block; margin: auto;" /&gt;
]
---

# Fitted models can also be used to predict new data


```r
alz_new &lt;- 
  tibble(tau = c(5, 6, 7), 
         VEGF = c(15, 15, 15),
         Class = c("Control", "Control", "Impaired")) %&gt;% 
  mutate(Class = factor(Class, levels = c("Impaired", "Control")))
alz_new
# # A tibble: 3 x 3
#     tau  VEGF Class   
#   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   
# 1     5    15 Control 
# 2     6    15 Control 
# 3     7    15 Impaired
```



---
class: center, middle

# Show of hands

How many people have used a model to generate .display[predictions] with R?

---

# Predict old data

.pull-right[


```r
lr_mod %&gt;% 
  fit(Class ~ tau + VEGF, 
      data = alz) %&gt;% 
  predict(new_data = alz) %&gt;% 
  mutate(true_class = alz$Class) %&gt;% 
  accuracy(truth = true_class, estimate = .pred_class)
# # A tibble: 1 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.859
```


]

---

# Predict new data

.pull-left[
## out with the old...

```r
lr_mod %&gt;% 
  fit(Class ~ tau + VEGF, 
      data = alz) %&gt;% 
  predict(new_data = alz) %&gt;% 
  mutate(true_class = alz$Class) %&gt;% 
  accuracy(truth = true_class, estimate = .pred_class)
# # A tibble: 1 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.859
```

]

.pull-right[

## in with the ðŸ†•


```r
lr_mod %&gt;% 
  fit(Class ~ tau + VEGF, 
      data = alz) %&gt;% 
* predict(new_data = alz_new) %&gt;%
* mutate(true_class = alz_new$Class) %&gt;%
  accuracy(truth = true_class, estimate = .pred_class)
# # A tibble: 1 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.667
```

]
---
class: middle, center

# `fit()`

Train a model by fitting a model. Returns a parsnip model fit.


```r
fit(lr_mod, Class ~ tau + VEGF, data = alz)
```


---
class: middle

.center[
# `fit()`

Train a model by fitting a model. Returns a parsnip model fit.
]


```r
lr_mod %&gt;%                     # parsnip model
  fit(Class ~ tau + VEGF,        # a formula
      data = alz               # dataframe
  )
```

---
class: middle

.center[
# `fit()`

Train a model by fitting a model. Returns a parsnip model fit.
]


```r
lr_fit &lt;-
  lr_mod %&gt;%                   # parsnip model
  fit(Class ~ tau + VEGF,        # a formula
      data = alz               # dataframe
  )
```

---
name: handout
class: center, middle

data `(x, y)` + model = fitted model


---
template: step1

---
name: step2
background-image: url("images/predicting/predicting.003.jpeg")
background-size: contain

---
class: middle, center

# `predict()`

Use a fitted model to predict new `y` values from data. Returns a tibble.


```r
predict(lr_fit, new_data = alz_new) 
```

---



```r
lr_fit %&gt;% 
  predict(new_data = alz_new)
# # A tibble: 3 x 1
#   .pred_class
#   &lt;fct&gt;      
# 1 Control    
# 2 Impaired   
# 3 Impaired
```

---
exclude: true
name: lm-predict
class: middle, center

# Predictions

&lt;img src="figs/rmed01-model/lm-predict-1.png" width="504" style="display: block; margin: auto;" /&gt;


---
class: middle, center, frame

# Axiom

The best way to measure a model's performance at predicting new data is to .display[predict new data].

---
class: middle, center, frame

# Data splitting


--


&lt;img src="figs/rmed01-model/all-split-1.png" width="864" style="display: block; margin: auto;" /&gt;

???


We refer to the group for which we know the outcome, and use to develop the algorithm, as the training set. We refer to the group for which we pretend we donâ€™t know the outcome as the test set.

---
class: inverse, middle, center

# Resample models

--

# with rsample


???

Enter the rsample package


---
class: middle, center, frame

# rsample

&lt;iframe src="https://tidymodels.github.io/rsample/" width="100%" height="400px"&gt;&lt;/iframe&gt;

---
class: center, middle

# `initial_split()*`

"Splits" data randomly into a single testing and a single training set.


```r
initial_split(data, prop = 3/4)
```

.footnote[`*` from `rsample`]
---


```r
ames_split &lt;- initial_split(ames, prop = 0.75)
ames_split
# &lt;Analysis/Assess/Total&gt;
# &lt;2198/732/2930&gt;
```

???

data splitting

---
class: center, middle

# `training()` and `testing()*`

Extract training and testing sets from an rsplit


```r
training(ames_split)
testing(ames_split)
```

.footnote[`*` from `rsample`]

---

```r
train_set &lt;- training(ames_split) 
train_set
# # A tibble: 2,198 x 81
#    MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley
#    &lt;fct&gt;       &lt;fct&gt;            &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;
#  1 One_Story_â€¦ Residentâ€¦          141    31770 Pave   No_Aâ€¦
#  2 One_Story_â€¦ Residentâ€¦           80    11622 Pave   No_Aâ€¦
#  3 One_Story_â€¦ Residentâ€¦           81    14267 Pave   No_Aâ€¦
#  4 One_Story_â€¦ Residentâ€¦           93    11160 Pave   No_Aâ€¦
#  5 Two_Story_â€¦ Residentâ€¦           74    13830 Pave   No_Aâ€¦
#  6 Two_Story_â€¦ Residentâ€¦           78     9978 Pave   No_Aâ€¦
#  7 One_Story_â€¦ Residentâ€¦           41     4920 Pave   No_Aâ€¦
#  8 One_Story_â€¦ Residentâ€¦           43     5005 Pave   No_Aâ€¦
#  9 One_Story_â€¦ Residentâ€¦           39     5389 Pave   No_Aâ€¦
# 10 Two_Story_â€¦ Residentâ€¦           60     7500 Pave   No_Aâ€¦
# # â€¦ with 2,188 more rows, and 75 more variables:
# #   Lot_Shape &lt;fct&gt;, Land_Contour &lt;fct&gt;, Utilities &lt;fct&gt;,
# #   Lot_Config &lt;fct&gt;, Land_Slope &lt;fct&gt;, â€¦
```


---
class: middle, center

# Quiz

Now that we have training and testing sets...

--

Which dataset do you think we use for .display[fitting]?

--

Which do we use for .display[predicting]?

---
template: step1

---
template: step2

---
template: step3
background-image: url("images/predicting/predicting.004.jpeg")
background-size: contain

---
name: holdout-step2
background-image: url("images/predicting/predicting.006.jpeg")
background-size: contain

---
name: holdout-step3
background-image: url("images/predicting/predicting.007.jpeg")
background-size: contain

---
name: holdout-step4
background-image: url("images/predicting/predicting.008.jpeg")
background-size: contain

---
name: holdout
background-image: url("images/predicting/predicting.009.jpeg")
background-size: contain

---
class: your-turn

# Your turn 2

Fill in the blanks. 

Use `initial_split()`, `training()`, `testing()`, `lm()` and `rmse()` to:

1. Split **ames** into training and test sets. Save the rsplit!

1. Extract the training data. Fit a linear model to it. Save the model!

1. Measure the RMSE of your linear model with your test set.  

Keep `set.seed(100)` at the start of your code.

<div class="countdown" id="timer_5f472e5b" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">04</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---


```r
set.seed(100) # Important!

alz_split  &lt;- initial_split(alz, prop = .9)
alz_train  &lt;- training(alz_split)
alz_test   &lt;- testing(alz_split)

nn_mod %&gt;% 
  fit(Class ~ tau + VEGF, 
      data = alz_train) %&gt;% 
  predict(new_data = alz_test) %&gt;% 
  mutate(true_class = alz_test$Class) %&gt;% 
  accuracy(truth = true_class, estimate = .pred_class)
```


---
template: handout

--

data `(x)` + fitted model = predictions

---
template: predictions

---
name: accurate-predictions
class: middle, center, frame

# Goal of Machine Learning

## ðŸŽ¯ generate .display[accurate predictions]

???

Now we have predictions from our model. What can we do with them? If we already know the truth, that is, the outcome variable that was observed, we can compare them!

---
class: middle, center, frame

# Axiom

Better Model = Better Predictions (Lower error rate)

---
class: middle, center

# `accuracy()*`

Calculates the accuracy based on two columns in a dataframe: 

The .display[truth]: `\({y}_i\)` 

The predicted .display[estimate]: `\(\hat{y}_i\)` 


```r
accuracy(data, truth, estimate)
```


.footnote[`*` from `yardstick`]

---


```r
lr_mod %&gt;% 
  fit(Class ~ tau + VEGF, 
      data = alz_train) %&gt;% 
  predict(new_data = alz_test) %&gt;% 
  mutate(true_class = alz_test$Class) %&gt;% 
* accuracy(truth = true_class, estimate = .pred_class)
# # A tibble: 1 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.909
```


---
template: step1

---
template: step2

---
name: step3
background-image: url("images/predicting/predicting.004.jpeg")
background-size: contain


---
class: your-turn

# Your Turn 3

What would happen if you repeated this process? Would you get the same answers?

Note your accuracy from above. Then change your seed number and rerun just the last code chunk above. Do you get the same answer? 

Try it a few times with a few different seeds.

<div class="countdown" id="timer_5f472eea" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---

.pull-left[


```
# # A tibble: 1 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.848
```


```
# # A tibble: 1 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.848
```


```
# # A tibble: 1 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.848
```

]

--

.pull-right[

```
# # A tibble: 1 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.848
```


```
# # A tibble: 1 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.848
```


```
# # A tibble: 1 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.848
```

]

---
class: middle, center

# Quiz

Why is the new estimate different?




---
class: middle, center

# Data Splitting

--

&lt;img src="figs/rmed01-model/unnamed-chunk-46-1.png" width="720" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-47-1.png" width="720" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-48-1.png" width="720" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-49-1.png" width="720" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-50-1.png" width="720" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-51-1.png" width="720" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-52-1.png" width="720" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-53-1.png" width="720" style="display: block; margin: auto;" /&gt;

---


&lt;img src="figs/rmed01-model/unnamed-chunk-54-1.png" width="1080" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-55-1.png" width="1080" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-56-1.png" width="1080" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-57-1.png" width="1080" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-58-1.png" width="1080" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-59-1.png" width="1080" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-60-1.png" width="1080" style="display: block; margin: auto;" /&gt;

--

&lt;img src="figs/rmed01-model/unnamed-chunk-61-1.png" width="1080" style="display: block; margin: auto;" /&gt;

--

.right[Mean RMSE]

---
class: frame, center, middle

# Resampling

Let's resample 10 times 

then compute the mean of the results...

---







```r
acc %&gt;% tibble::enframe(name = "accuracy")
# # A tibble: 10 x 2
#    accuracy value
#       &lt;int&gt; &lt;dbl&gt;
#  1        1 0.855
#  2        2 0.807
#  3        3 0.831
#  4        4 0.855
#  5        5 0.880
#  6        6 0.880
#  7        7 0.831
#  8        8 0.843
#  9        9 0.880
# 10       10 0.892
mean(acc)
# [1] 0.8554217
```

---
class: middle, center

# Guess

Which do you think is a better estimate?

The best result or the mean of the results? Why? 

---
class: middle, center

# But also...

Fit with .display[training set]

Predict with .display[testing set]

--

Rinse and repeat?

---

# There has to be a better way...


```r
acc &lt;- vector(length = 10, mode = "double")
for (i in 1:10) {
  new_split &lt;- initial_split(alz)
  new_train &lt;- training(new_split)
  new_test  &lt;- testing(new_split)
  acc[i] &lt;-
    lr_mod %&gt;% 
      fit(Class ~ tau + VEGF, data = new_train) %&gt;% 
      predict(new_test) %&gt;% 
      mutate(truth = new_test$Class) %&gt;% 
      accuracy(truth, .pred_class) %&gt;% 
      pull(.estimate)
}
```

---
background-image: url(images/diamonds.jpg)
background-size: contain
background-position: left
class: middle, center
background-color: #f5f5f5

.pull-right[
## The .display[testing set] is precious...

## we can only use it once!

]

---
background-image: url(https://www.tidymodels.org/start/resampling/img/resampling.svg)
background-size: 60%

---
class: middle, center, inverse

# Cross-validation

---
background-image: url(images/cross-validation/Slide2.png)
background-size: contain

---
background-image: url(images/cross-validation/Slide3.png)
background-size: contain

---
background-image: url(images/cross-validation/Slide4.png)
background-size: contain

---
background-image: url(images/cross-validation/Slide5.png)
background-size: contain

---
background-image: url(images/cross-validation/Slide6.png)
background-size: contain

---
background-image: url(images/cross-validation/Slide7.png)
background-size: contain

---
background-image: url(images/cross-validation/Slide8.png)
background-size: contain

---
background-image: url(images/cross-validation/Slide9.png)
background-size: contain

---
background-image: url(images/cross-validation/Slide10.png)
background-size: contain

---
background-image: url(images/cross-validation/Slide11.png)
background-size: contain

---
class: middle, center

# V-fold cross-validation


```r
vfold_cv(data, v = 10, ...)
```


---
exclude: true



---
class: middle, center

# Guess

How many times does in observation/row appear in the assessment set?

&lt;img src="figs/rmed01-model/vfold-tiles-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

&lt;img src="figs/rmed01-model/unnamed-chunk-66-1.png" width="864" style="display: block; margin: auto;" /&gt;

---
class: middle, center

# Quiz

If we use 10 folds, which percent of our data will end up in the training set and which percent in the testing set for each fold?

--

90% - training

10% - test

---
class: your-turn

# Your Turn 4

Run the code below. What does it return?


```r
set.seed(100)
alz_folds &lt;- 
    vfold_cv(alz_train, v = 10, strata = Class)
alz_folds
```

<div class="countdown" id="timer_5f472c82" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">01</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---

```r
set.seed(100)
alz_folds &lt;- 
    vfold_cv(alz_train, v = 10, strata = Class)
alz_folds
# #  10-fold cross-validation using stratification 
# # A tibble: 10 x 2
#    splits           id    
#    &lt;list&gt;           &lt;chr&gt; 
#  1 &lt;split [269/31]&gt; Fold01
#  2 &lt;split [269/31]&gt; Fold02
#  3 &lt;split [270/30]&gt; Fold03
#  4 &lt;split [270/30]&gt; Fold04
#  5 &lt;split [270/30]&gt; Fold05
#  6 &lt;split [270/30]&gt; Fold06
#  7 &lt;split [270/30]&gt; Fold07
#  8 &lt;split [270/30]&gt; Fold08
#  9 &lt;split [271/29]&gt; Fold09
# 10 &lt;split [271/29]&gt; Fold10
```

---
class: middle

.center[
# We need a new way to fit
]


```r
split1       &lt;- cv_folds %&gt;% pluck("splits", 1)
split1_train &lt;- training(split1)
split1_test  &lt;- testing(split1)

rt_spec %&gt;% 
  fit(Sale_Price ~ ., data = split1_train) %&gt;% 
  predict(split1_test) %&gt;% 
  mutate(truth = split1_test$Sale_Price) %&gt;% 
  rmse(truth, .pred)

# rinse and repeat
split2 &lt;- ...
```


---
class: middle

.center[
# `fit_resamples()`

Trains and tests a resampled model.
]


```r
lr_mod %&gt;% 
  fit_resamples(
    Class ~ tau + VEGF, 
    resamples = alz_folds
    )
```

---


```r
lr_mod %&gt;% 
  fit_resamples(
    Class ~ tau + VEGF, 
    resamples = alz_folds
    )
# # Resampling results
# # 10-fold cross-validation using stratification 
# # A tibble: 10 x 4
#    splits           id     .metrics         .notes          
#    &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          
#  1 &lt;split [269/31]&gt; Fold01 &lt;tibble [2 Ã— 3]&gt; &lt;tibble [0 Ã— 1]&gt;
#  2 &lt;split [269/31]&gt; Fold02 &lt;tibble [2 Ã— 3]&gt; &lt;tibble [0 Ã— 1]&gt;
#  3 &lt;split [270/30]&gt; Fold03 &lt;tibble [2 Ã— 3]&gt; &lt;tibble [0 Ã— 1]&gt;
#  4 &lt;split [270/30]&gt; Fold04 &lt;tibble [2 Ã— 3]&gt; &lt;tibble [0 Ã— 1]&gt;
#  5 &lt;split [270/30]&gt; Fold05 &lt;tibble [2 Ã— 3]&gt; &lt;tibble [0 Ã— 1]&gt;
#  6 &lt;split [270/30]&gt; Fold06 &lt;tibble [2 Ã— 3]&gt; &lt;tibble [0 Ã— 1]&gt;
#  7 &lt;split [270/30]&gt; Fold07 &lt;tibble [2 Ã— 3]&gt; &lt;tibble [0 Ã— 1]&gt;
#  8 &lt;split [270/30]&gt; Fold08 &lt;tibble [2 Ã— 3]&gt; &lt;tibble [0 Ã— 1]&gt;
#  9 &lt;split [271/29]&gt; Fold09 &lt;tibble [2 Ã— 3]&gt; &lt;tibble [0 Ã— 1]&gt;
# 10 &lt;split [271/29]&gt; Fold10 &lt;tibble [2 Ã— 3]&gt; &lt;tibble [0 Ã— 1]&gt;
```


---
class: middle, center

# `collect_metrics()`

Unnest the metrics column from a tidymodels `fit_resamples()`


```r
_results %&gt;% collect_metrics(summarize = TRUE)
```

--

.footnote[`TRUE` is actually the default; averages across folds]

---

```r
lr_mod %&gt;% 
  fit_resamples(
    Class ~ tau + VEGF, 
    resamples = alz_folds
    ) %&gt;% 
  collect_metrics(summarize = FALSE)
# # A tibble: 20 x 4
#    id     .metric  .estimator .estimate
#    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
#  1 Fold01 accuracy binary         0.806
#  2 Fold01 roc_auc  binary         0.859
#  3 Fold02 accuracy binary         0.806
#  4 Fold02 roc_auc  binary         0.793
#  5 Fold03 accuracy binary         0.867
#  6 Fold03 roc_auc  binary         0.932
#  7 Fold04 accuracy binary         0.8  
#  8 Fold04 roc_auc  binary         0.773
#  9 Fold05 accuracy binary         0.933
# 10 Fold05 roc_auc  binary         0.960
# # â€¦ with 10 more rows
```

---
class: middle, center, frame

# 10-fold CV

### 10 different analysis/assessment sets

### 10 different models (trained on .display[analysis] sets)

### 10 different sets of performance statistics (on .display[assessment] sets)



---
class: your-turn

# Your Turn 5

Modify the code below to use `fit_resamples` and `alz_folds` to cross-validate the KNN model. What is the ROC AUC that you collect at the end?


```r
set.seed(100)
lr_mod %&gt;% 
  fit(Class ~ tau + VEGF, 
      data = alz_train) %&gt;% 
  predict(new_data = alz_test) %&gt;% 
  mutate(true_class = alz_test$Class) %&gt;% 
  accuracy(truth = true_class, estimate = .pred_class)
```

<div class="countdown" id="timer_5f472d1b" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>


---

```r
set.seed(100)
lr_mod %&gt;% 
  fit_resamples(Class ~ tau + VEGF, 
                resamples = alz_folds) %&gt;% 
  collect_metrics()
# # A tibble: 2 x 5
#   .metric  .estimator  mean     n std_err
#   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
# 1 accuracy binary     0.850    10  0.0155
# 2 roc_auc  binary     0.885    10  0.0251
```

---

# How did we do?


```r
lr_mod %&gt;% 
  fit(Class ~ tau + VEGF, data = alz_train) %&gt;% 
  predict(alz_test) %&gt;% 
  mutate(truth = alz_test$Class) %&gt;% 
  accuracy(truth, .pred_class)
# # A tibble: 1 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.909
```



```
# # A tibble: 2 x 5
#   .metric  .estimator  mean     n std_err
#   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
# 1 accuracy binary     0.850    10  0.0155
# 2 roc_auc  binary     0.885    10  0.0251
```

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "r",
"highlightStyle": "xcode",
"slideNumberFormat": "",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
